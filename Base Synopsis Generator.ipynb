{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e1e40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates a (k, lambda, eta, beta)- base synopsis generator that is (epsilon,delta)-DP \n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sympy as sp\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "lower_bound = -1 # data lower bound\n",
    "upper_bound = 1  # data upper bound\n",
    "data_precision = 2 # data precision\n",
    "\n",
    "n = 200 \n",
    "num_points = 2*n # number of data points\n",
    "m = num_points*math.log(((upper_bound - lower_bound)/10**(-data_precision)) +1) # size of the domain universe\n",
    "\n",
    "delta =  0.3 # DP parameter\n",
    "eta = 0.01 # edge for boosting\n",
    "beta = 0.2 # failure probability of the base synopsis\n",
    "k = math.ceil(2*((math.log(2/beta)+m)/(1-2*eta))) # number of query sample as demanded by Lemma 6.5 \n",
    "# Assume coefficient \\in (0,1], changing one x_i can at most change 1*[(1+x_j)^2 - (-1+x_j)^2] = 4x_j <= 4.\n",
    "\n",
    "rho = 4/num_points-2/num_points**2 # l_1 sensitivity of our query = 4 sub\n",
    "# rho = 16\n",
    "\n",
    "\n",
    "\n",
    "Llambda = 0.4 # accuracy parameter lambda\n",
    "epsilon = (math.log(1/beta)*rho*math.sqrt(k*math.log(1/delta)))/Llambda\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4278ed3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9028448235520354"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8261b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query does the following:\n",
    "# Given a database X, it selects two points x_i, x_j \\in X uniformly at random and computes\n",
    "# q_c(X) = c(x_i + x_j)^2.\n",
    "# The set of queries Q consists of all such q_c with coefficient c \\in (0, 1] \n",
    "# Note that we are not setting the precision of coefficients to allow a large size of Q\n",
    "\n",
    "c, x_i, x_j = sp.symbols('c x_i x_j') \n",
    "# c, x_1, x_2, x_3, x_4, x_5  = sp.symbols('c x_1, x_2, x_3, x_4, x_5')\n",
    "\n",
    "# deg_2_poly = Function('deg_2_poly')\n",
    "\n",
    "deg_2_poly = c*(x_i+x_j)**2 \n",
    "# deg_2_poly = c*(x_1+ x_2+ x_3+ x_4+ x_5)**2\n",
    "\n",
    "gradient_deg_2_poly = sp.diff(deg_2_poly, x_i)\n",
    "hession_deg_2_poly = sp.diff(gradient_deg_2_poly, x_i) \n",
    "# gradient_deg_2_poly = sp.diff(deg_2_poly, x_1)\n",
    "# hession_deg_2_poly = sp.diff(gradient_deg_2_poly, x_1)\n",
    "\n",
    "q_hat = sp.symbols('q_hat')\n",
    "\n",
    "gradient_loss_per_q = (1/Llambda**2)*2*(deg_2_poly-q_hat)*gradient_deg_2_poly\n",
    "hession_loss_per_q = (1/Llambda**2)*(2*(gradient_deg_2_poly)**2+2*(deg_2_poly-q_hat)*hession_deg_2_poly)\n",
    "\n",
    "loss_per_q = (1/Llambda**2)*(deg_2_poly-q_hat)**2\n",
    "# evaluate a function\n",
    "# deg_2_poly.evalf(4, subs={c:1, x_i:2.32, x_j:3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d61310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all possible pairs of data among the given data points\n",
    "all_possible_pairs= np.array(list(combinations(range(num_points), 2))) \n",
    "# all_possible_pairs= np.array(list(combinations(range(num_points), 5)))\n",
    "\n",
    "# The entire query set Q corresponds to a coefficient_array of (0,1] of precision 0.001\n",
    "all_coeff = 1-np.linspace(0, 1, num=len(all_possible_pairs), endpoint=False, dtype=None)\n",
    "\n",
    "# for each query, we also need to fix a pair of x_i, x_j\n",
    "UAR_among_all_pairs= np.random.choice(range(len(all_possible_pairs)), size=len(all_coeff), replace=False)\n",
    "\n",
    "# the k-th row stores a pair of indices (i,j) used by q_k = c_k(x_i+x_j)^2\n",
    "all_pairs_indices = all_possible_pairs[UAR_among_all_pairs,:] \n",
    "\n",
    "# create a dataframe to store the coefficient and pairs indices for each query in Q\n",
    "df_deg_2_poly = {'coefficient': all_coeff, 'pairs indices 1': all_pairs_indices[:,0],'pairs indices 2': all_pairs_indices[:,1]} \n",
    "# df_deg_2_poly = {'coefficient': all_coeff, 'pairs indices 1': all_pairs_indices[:,0],'pairs indices 2': all_pairs_indices[:,1], \n",
    "                #  'pairs indices 3': all_pairs_indices[:,2],'pairs indices 4': all_pairs_indices[:,3], \n",
    "                #  'pairs indices 5': all_pairs_indices[:,4],'pairs indices 6': all_pairs_indices[:,5]}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_deg_2_poly = pd.DataFrame(df_deg_2_poly).apply(lambda x: round(x, data_precision+2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e65282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on a set of real data \n",
    "real_X = np.random.uniform(low=lower_bound, high=upper_bound, size=num_points)\n",
    "# initialize the synopsis to be some arbirary set of data, say from the standard normal\n",
    "# fake_X = np.random.randn(num_points)\n",
    "fake_X = copy.copy(real_X)\n",
    "fake_X_copy = copy.copy(fake_X) # save a copy of fake_X\n",
    "\n",
    "# for verification purposes, if fake_X = real_X, the initial error should be the same as the added laplace noise\n",
    "# fake_X = real_X \n",
    "\n",
    "# To initialize boosting, we choose UAR a size k subset of Q \n",
    "# initially, queries are sampled UAR. This distribution will change after each iteration of boosting\n",
    "df_sampled_queries = df_deg_2_poly.sample(n=k, replace=False, weights=None, axis=None)\n",
    "sampled_queries_dict = df_sampled_queries.reset_index().to_dict()\n",
    "#### BOOSTING LOOP STARTS ####\n",
    "\n",
    "# initialize all-zero arrays to store noiselss query output, noisy output, and laplace noise  \n",
    "real_output = np.zeros(k)\n",
    "real_data_noisy_output = np.zeros(k)\n",
    "lap_noise = np.zeros(k)\n",
    "fake_data_output = np.zeros(k)\n",
    "error = np.zeros(k) # store |q(X) - noisy_output| for each q\n",
    "\n",
    "\n",
    "# for each query, compute its real output, noisy output, and error\n",
    "for index, coefficient in enumerate(df_sampled_queries['coefficient']):\n",
    "    # store the original index \n",
    "    ori_idx_queries = df_sampled_queries['coefficient'].index\n",
    "    # store the real x_i, x_j used for each query\n",
    "    real_xi = real_X[df_sampled_queries['pairs indices 1'][ori_idx_queries[index]]]\n",
    "    real_xj = real_X[df_sampled_queries['pairs indices 2'][ori_idx_queries[index]]]\n",
    "    # real output\n",
    "    real_output[index] = deg_2_poly.evalf(data_precision+2, subs={c:df_sampled_queries['coefficient'][ori_idx_queries[index]], x_i:real_xi, x_j:real_xj})\n",
    "\n",
    "    \n",
    "    # compute noisy output on the real data\n",
    "    lap_noise[index] = np.random.laplace(loc=0.0, scale=rho*(2*math.sqrt(2*k*math.log(1/delta))/epsilon), size=None) \n",
    "    # lap_noise[index] = 0\n",
    "    real_data_noisy_output[index] = real_output[index] + lap_noise[index]\n",
    "    \n",
    "    # compute query output on the current synopsis \n",
    "    fake_xi = fake_X[df_sampled_queries['pairs indices 1'][ori_idx_queries[index]]]\n",
    "    fake_xj = fake_X[df_sampled_queries['pairs indices 2'][ori_idx_queries[index]]]\n",
    "    \n",
    "    # compute query output on fake data \n",
    "    fake_data_output[index] = deg_2_poly.evalf(data_precision+4, subs={c:df_sampled_queries['coefficient'][ori_idx_queries[index]], x_i:fake_xi, x_j:fake_xj})\n",
    " \n",
    "    # calculate initial error\n",
    "    # notice that this is |q(X) - real_data_noisy_output|\n",
    "    error[index] = abs(fake_data_output[index]-real_data_noisy_output[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ca6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7700b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#iter 0 x_co=250, 1st=-95.21737670898438, 2nd=199.989, # queries below=262 fake x=-0.3836529706613294\n",
      "Total loss =  1122.44\n",
      "#iter 1 x_co=302, 1st=56.009254455566406, 2nd=138.471, # queries below=261 fake x=0.5506466854612742\n",
      "Total loss =  1095.05\n",
      "#iter 2 x_co=152, 1st=54.56489562988281, 2nd=26.8783, # queries below=262 fake x=-2.291702159907795\n",
      "Total loss =  1081.92\n",
      "#iter 3 x_co=393, 1st=-580.3302001953125, 2nd=789.279, # queries below=260 fake x=-0.2099780648235119\n",
      "Total loss =  1382.51\n",
      "#iter 4 x_co=152, 1st=-186.23306274414062, 2nd=386.952, # queries below=259 fake x=-1.810420662173963\n",
      "Total loss =  1127.96\n",
      "#iter 5 x_co=123, 1st=48.26048278808594, 2nd=-49.5918, # queries below=259 fake x=0.5989809599187121\n",
      "Total loss =  1076.54\n",
      "#iter 6 x_co=152, 1st=-40.48711395263672, 2nd=224.456, # queries below=260 fake x=-1.6300420412677235\n",
      "Total loss =  1091.59\n",
      "#iter 7 x_co=270, 1st=37.12384033203125, 2nd=79.3091, # queries below=260 fake x=-0.22068775594824053\n",
      "Total loss =  1087.65\n",
      "#iter 8 x_co=279, 1st=-35.84724426269531, 2nd=-29.3654, # queries below=259 fake x=-1.108920258512331\n",
      "Total loss =  1078.85\n",
      "#iter 9 x_co=123, 1st=45.99601745605469, 2nd=-59.6249, # queries below=260 fake x=1.3704034664907492\n",
      "Total loss =  1099.38\n",
      "#iter 10 x_co=229, 1st=52.87294006347656, 2nd=65.3369, # queries below=260 fake x=-0.31857849232631463\n",
      "Total loss =  1120.57\n",
      "#iter 11 x_co=279, 1st=-48.78931427001953, 2nd=4.20826, # queries below=260 fake x=10.484783179905957\n",
      "Total loss =  1095.49\n",
      "#iter 12 x_co=279, 1st=30634.1484375, 2nd=8172.11, # queries below=260 fake x=6.736160166422746\n",
      "Total loss =  86904.3\n",
      "#iter 13 x_co=279, 1st=9062.04296875, 2nd=3638.67, # queries below=260 fake x=4.245679222051944\n",
      "Total loss =  17810.0\n",
      "#iter 14 x_co=279, 1st=2671.052734375, 2nd=1626.71, # queries below=260 fake x=2.6036841595176377\n",
      "Total loss =  4239.41\n",
      "#iter 15 x_co=279, 1st=778.037353515625, 2nd=736.873, # queries below=261 fake x=1.5478201950833856\n",
      "Total loss =  1607.64\n",
      "#iter 16 x_co=279, 1st=217.93148803710938, 2nd=347.985, # queries below=261 fake x=0.9215531935010084\n",
      "Total loss =  1117.97\n",
      "#iter 17 x_co=279, 1st=53.634254455566406, 2nd=185.116, # queries below=262 fake x=0.6318195689558657\n",
      "Total loss =  1038.25\n",
      "#iter 18 x_co=336, 1st=31.631431579589844, 2nd=104.672, # queries below=262 fake x=0.4061280235696645\n",
      "Total loss =  1029.63\n",
      "#iter 19 x_co=250, 1st=-28.74903106689453, 2nd=87.8079, # queries below=263 fake x=-0.056244650774970095\n",
      "Total loss =  1023.96\n",
      "#iter 20 x_co=353, 1st=-27.926403045654297, 2nd=92.3321, # queries below=263 fake x=-0.5831131742758406\n",
      "Total loss =  1018.30\n",
      "#iter 21 x_co=254, 1st=-25.606273651123047, 2nd=158.834, # queries below=264 fake x=-0.7105108377723641\n",
      "Total loss =  1013.23\n",
      "#iter 22 x_co=149, 1st=25.47867202758789, 2nd=-3.16443, # queries below=263 fake x=7.418879956397019\n",
      "Total loss =  1011.00\n",
      "#iter 23 x_co=149, 1st=8639.263671875, 2nd=3618.47, # queries below=263 fake x=5.031332870871166\n",
      "Total loss =  16328.2\n",
      "#iter 24 x_co=149, 1st=2539.3349609375, 2nd=1621.07, # queries below=264 fake x=3.4648805877067854\n",
      "Total loss =  3932.29\n",
      "#iter 25 x_co=149, 1st=738.8740234375, 2nd=733.556, # queries below=264 fake x=2.457630731663995\n",
      "Total loss =  1546.19\n",
      "#iter 26 x_co=255, 1st=224.20736694335938, 2nd=334.019, # queries below=263 fake x=-0.6196228873468668\n",
      "Total loss =  1101.65\n",
      "#iter 27 x_co=149, 1st=48.428138732910156, 2nd=172.374, # queries below=263 fake x=2.176683327576681\n",
      "Total loss =  1017.92\n",
      "#iter 28 x_co=353, 1st=-36.244163513183594, 2nd=125.800, # queries below=264 fake x=-0.295003517162687\n",
      "Total loss =  1010.24\n",
      "#iter 29 x_co=5, 1st=-25.396377563476562, 2nd=-9.66820, # queries below=262 fake x=-3.305365340110491\n",
      "Total loss =  1004.21\n",
      "#iter 30 x_co=5, 1st=-1088.27294921875, 2nd=1181.21, # queries below=261 fake x=-2.384047856714524\n",
      "Total loss =  1782.14\n",
      "#iter 31 x_co=5, 1st=-326.9560546875, 2nd=516.024, # queries below=261 fake x=-1.7504417830783225\n",
      "Total loss =  1177.25\n",
      "#iter 32 x_co=5, 1st=-102.44003295898438, 2nd=213.747, # queries below=261 fake x=-1.2711838284521448\n",
      "Total loss =  1051.33\n",
      "#iter 33 x_co=5, 1st=-37.547584533691406, 2nd=69.1164, # queries below=264 fake x=-0.7279324617359109\n",
      "Total loss =  1020.56\n",
      "#iter 34 x_co=5, 1st=-24.973690032958984, 2nd=-7.32971, # queries below=261 fake x=-4.135120413652423\n",
      "Total loss =  1005.45\n",
      "#iter 35 x_co=5, 1st=-2396.8896484375, 2nd=2009.16, # queries below=261 fake x=-2.9421391710585496\n",
      "Total loss =  3180.55\n",
      "#iter 36 x_co=5, 1st=-713.91015625, 2nd=887.035, # queries below=261 fake x=-2.1373112900029057\n",
      "Total loss =  1458.08\n",
      "#iter 37 x_co=5, 1st=-216.40518188476562, 2nd=383.277, # queries below=261 fake x=-1.5726928214028035\n",
      "Total loss =  1110.89\n",
      "#iter 38 x_co=5, 1st=-70.11170959472656, 2nd=151.664, # queries below=262 fake x=-1.1104104621218411\n",
      "Total loss =  1036.16\n",
      "#iter 39 x_co=5, 1st=-29.141902923583984, 2nd=36.8065, # queries below=264 fake x=-0.31864985725609785\n",
      "Total loss =  1015.27\n",
      "#iter 40 x_co=5, 1st=-28.992942810058594, 2nd=-3.51552, # queries below=261 fake x=-8.565785600132351\n",
      "Total loss =  994.357\n",
      "#iter 41 x_co=5, 1st=-26942.4296875, 2nd=10101.4, # queries below=261 fake x=-5.898588251072731\n",
      "Total loss =  54938.8\n",
      "#iter 42 x_co=5, 1st=-7982.6640625, 2nd=4489.11, # queries below=261 fake x=-4.12035939158806\n",
      "Total loss =  11689.9\n",
      "#iter 43 x_co=5, 1st=-2367.35498046875, 2nd=1992.54, # queries below=261 fake x=-2.9322476369840658\n",
      "Total loss =  3145.39\n",
      "#iter 44 x_co=5, 1st=-705.1727294921875, 2nd=879.605, # queries below=261 fake x=-2.1305548736107696\n",
      "Total loss =  1451.06\n",
      "#iter 45 x_co=5, 1st=-213.82696533203125, 2nd=379.912, # queries below=261 fake x=-1.5677214729415923\n",
      "Total loss =  1109.44\n",
      "#iter 46 x_co=5, 1st=-69.3616943359375, 2nd=150.071, # queries below=262 fake x=-1.105529024574274\n",
      "Total loss =  1035.81\n",
      "#iter 47 x_co=5, 1st=-28.964317321777344, 2nd=35.9528, # queries below=264 fake x=-0.2999092198843627\n",
      "Total loss =  1015.12\n",
      "#iter 48 x_co=5, 1st=-29.045520782470703, 2nd=-2.07737, # queries below=261 fake x=-14.281786202040244\n",
      "Total loss =  993.814\n",
      "#iter 49 x_co=5, 1st=-135725.78125, 2nd=29676.8, # queries below=261 fake x=-9.70832778952316\n",
      "Total loss =  466546.\n",
      "#iter 50 x_co=5, 1st=-40209.6328125, 2nd=13191.2, # queries below=261 fake x=-6.66010195628845\n",
      "Total loss =  92964.7\n",
      "#iter 51 x_co=5, 1st=-11912.6640625, 2nd=5862.89, # queries below=261 fake x=-4.628227346676873\n",
      "Total loss =  19198.7\n",
      "#iter 52 x_co=5, 1st=-3531.11767578125, 2nd=2603.94, # queries below=262 fake x=-3.272160543422409\n",
      "Total loss =  4630.07\n",
      "#iter 53 x_co=5, 1st=-1049.527099609375, 2nd=1152.59, # queries below=261 fake x=-2.3615827986449673\n",
      "Total loss =  1746.65\n",
      "#iter 54 x_co=5, 1st=-315.50848388671875, 2nd=503.144, # queries below=261 fake x=-1.7345089156484264\n",
      "Total loss =  1170.04\n",
      "#iter 55 x_co=5, 1st=-99.08209228515625, 2nd=207.776, # queries below=262 fake x=-1.2576394842928593\n",
      "Total loss =  1049.73\n",
      "#iter 56 x_co=5, 1st=-36.632080078125, 2nd=66.0804, # queries below=264 fake x=-0.7032829761944512\n",
      "Total loss =  1020.05\n",
      "#iter 57 x_co=5, 1st=-25.170333862304688, 2nd=-8.59340, # queries below=261 fake x=-3.6323140315791633\n",
      "Total loss =  1004.84\n",
      "#iter 58 x_co=5, 1st=-1522.649658203125, 2nd=1481.56, # queries below=261 fake x=-2.6045768909596845\n",
      "Total loss =  2206.29\n",
      "#iter 59 x_co=5, 1st=-455.3450927734375, 2nd=650.903, # queries below=261 fake x=-1.905017423948884\n",
      "Total loss =  1262.97\n",
      "#iter 60 x_co=5, 1st=-140.18124389648438, 2nd=275.828, # queries below=261 fake x=-1.3967971107770367\n",
      "Total loss =  1069.96\n",
      "#iter 61 x_co=5, 1st=-48.118873596191406, 2nd=100.027, # queries below=264 fake x=-0.9157375400754617\n",
      "Total loss =  1025.90\n",
      "#iter 62 x_co=5, 1st=-24.917503356933594, 2nd=8.58327, # queries below=261 fake x=1.9872950504317615\n",
      "Total loss =  1010.09\n",
      "#iter 63 x_co=5, 1st=802.875244140625, 2nd=1004.20, # queries below=262 fake x=1.1877813080899173\n",
      "Total loss =  1440.09\n",
      "#iter 64 x_co=5, 1st=228.929443359375, 2nd=465.096, # queries below=263 fake x=0.6955611344180639\n",
      "Total loss =  1056.33\n",
      "#iter 65 x_co=5, 1st=60.166954040527344, 2nd=233.345, # queries below=264 fake x=0.43771540558172717\n",
      "Total loss =  989.863\n",
      "#iter 66 x_co=123, 1st=29.76068878173828, 2nd=265.164, # queries below=264 fake x=1.2581685895949195\n",
      "Total loss =  981.040\n",
      "#iter 67 x_co=370, 1st=24.513702392578125, 2nd=126.760, # queries below=265 fake x=-1.0186744315774146\n",
      "Total loss =  979.276\n",
      "#iter 68 x_co=344, 1st=23.501953125, 2nd=46.4081, # queries below=265 fake x=0.10873467516811774\n",
      "Total loss =  977.270\n",
      "#iter 69 x_co=284, 1st=-23.37521743774414, 2nd=214.732, # queries below=265 fake x=-0.8860701856175481\n",
      "Total loss =  969.708\n",
      "#iter 70 x_co=388, 1st=-23.304264068603516, 2nd=89.8824, # queries below=264 fake x=0.6383156738065532\n",
      "Total loss =  968.371\n",
      "#iter 71 x_co=393, 1st=21.808074951171875, 2nd=200.209, # queries below=263 fake x=-0.31890473978877276\n",
      "Total loss =  966.166\n",
      "#iter 72 x_co=152, 1st=-25.081340789794922, 2nd=202.929, # queries below=264 fake x=-1.5064456252261231\n",
      "Total loss =  965.024\n",
      "#iter 73 x_co=393, 1st=21.51117706298828, 2nd=190.650, # queries below=263 fake x=-0.4317353136645985\n",
      "Total loss =  963.384\n",
      "#iter 74 x_co=152, 1st=-22.821903228759766, 2nd=199.466, # queries below=264 fake x=-1.3920304073873973\n",
      "Total loss =  962.232\n",
      "#iter 75 x_co=169, 1st=-21.050552368164062, 2nd=58.4403, # queries below=264 fake x=1.1765642065134652\n",
      "Total loss =  960.856\n",
      "#iter 76 x_co=209, 1st=23.826904296875, 2nd=133.772, # queries below=265 fake x=0.12369534209373234\n",
      "Total loss =  958.954\n",
      "#iter 77 x_co=234, 1st=20.616893768310547, 2nd=73.1600, # queries below=266 fake x=0.6645073176337033\n",
      "Total loss =  956.579\n",
      "#iter 78 x_co=203, 1st=20.428993225097656, 2nd=67.7218, # queries below=266 fake x=-0.7832052229464344\n",
      "Total loss =  953.318\n",
      "#iter 79 x_co=113, 1st=20.060745239257812, 2nd=54.5507, # queries below=266 fake x=0.06657564299034568\n",
      "Total loss =  950.500\n",
      "#iter 80 x_co=365, 1st=-19.54937744140625, 2nd=4.88039, # queries below=263 fake x=4.299438460595366\n",
      "Total loss =  946.201\n",
      "#iter 81 x_co=365, 1st=2992.4130859375, 2nd=2275.24, # queries below=263 fake x=2.9842315423605443\n",
      "Total loss =  3864.62\n",
      "#iter 82 x_co=365, 1st=882.9677734375, 2nd=1016.22, # queries below=263 fake x=2.1153585286111367\n",
      "Total loss =  1497.64\n",
      "#iter 83 x_co=365, 1st=257.5548095703125, 2nd=459.897, # queries below=265 fake x=1.5553317637570787\n",
      "Total loss =  1037.16\n",
      "#iter 84 x_co=365, 1st=72.13291931152344, 2nd=217.465, # queries below=263 fake x=1.2236332890768695\n",
      "Total loss =  951.175\n",
      "#iter 85 x_co=239, 1st=29.626564025878906, 2nd=95.6445, # queries below=264 fake x=-0.21717502910098152\n",
      "Total loss =  937.219\n",
      "#iter 86 x_co=393, 1st=19.22376251220703, 2nd=190.028, # queries below=263 fake x=-0.5328980480857433\n",
      "Total loss =  931.780\n",
      "#iter 87 x_co=152, 1st=-20.15744400024414, 2nd=195.356, # queries below=264 fake x=-1.2888472749665092\n",
      "Total loss =  930.860\n",
      "#iter 88 x_co=73, 1st=18.99457550048828, 2nd=130.103, # queries below=264 fake x=0.8253216300194122\n",
      "Total loss =  929.769\n",
      "#iter 89 x_co=246, 1st=18.326053619384766, 2nd=74.4038, # queries below=264 fake x=0.31383663857735145\n",
      "Total loss =  928.241\n",
      "#iter 90 x_co=10, 1st=-17.76805877685547, 2nd=57.9319, # queries below=264 fake x=-0.3372134097331294\n",
      "Total loss =  925.724\n",
      "#iter 91 x_co=302, 1st=17.681312561035156, 2nd=94.7709, # queries below=265 fake x=0.3640777522406753\n",
      "Total loss =  922.485\n",
      "#iter 92 x_co=393, 1st=17.036758422851562, 2nd=194.148, # queries below=264 fake x=-0.6206496602912375\n",
      "Total loss =  920.764\n",
      "#iter 93 x_co=152, 1st=-17.163589477539062, 2nd=190.703, # queries below=265 fake x=-1.1988457616213233\n",
      "Total loss =  920.055\n",
      "#iter 94 x_co=255, 1st=16.375595092773438, 2nd=151.517, # queries below=265 fake x=-0.7277004722889329\n",
      "Total loss =  919.249\n",
      "#iter 95 x_co=328, 1st=16.162086486816406, 2nd=79.7653, # queries below=266 fake x=-1.0879217882073198\n",
      "Total loss =  918.356\n",
      "#iter 96 x_co=294, 1st=-15.909515380859375, 2nd=66.5349, # queries below=267 fake x=1.0172590118466707\n",
      "Total loss =  916.949\n",
      "#iter 97 x_co=295, 1st=-15.701705932617188, 2nd=16.2405, # queries below=268 fake x=0.8688118311804398\n",
      "Total loss =  915.526\n",
      "#iter 98 x_co=168, 1st=92.85256958007812, 2nd=185.040, # queries below=268 fake x=0.18509494296064144\n",
      "Total loss =  929.963\n",
      "#iter 99 x_co=168, 1st=27.07370376586914, 2nd=88.4967, # queries below=267 fake x=-0.12083411202994387\n",
      "Total loss =  901.899\n",
      "#iter 100 x_co=52, 1st=15.394479751586914, 2nd=4.20549, # queries below=266 fake x=-3.6316956600345724\n",
      "Total loss =  897.263\n",
      "#iter 101 x_co=52, 1st=-2183.14599609375, 2nd=1580.23, # queries below=266 fake x=-2.250158331216446\n",
      "Total loss =  3105.00\n",
      "#iter 102 x_co=52, 1st=-635.191162109375, 2nd=715.274, # queries below=266 fake x=-1.3621195703261355\n",
      "Total loss =  1295.76\n",
      "#iter 103 x_co=52, 1st=-180.09930419921875, 2nd=332.217, # queries below=267 fake x=-0.8200059698534607\n",
      "Total loss =  958.926\n",
      "#iter 104 x_co=52, 1st=-47.63164520263672, 2nd=164.896, # queries below=269 fake x=-0.531147774177227\n",
      "Total loss =  901.296\n",
      "#iter 105 x_co=190, 1st=15.234739303588867, 2nd=58.8756, # queries below=270 fake x=0.23466818477921558\n",
      "Total loss =  893.413\n",
      "#iter 106 x_co=143, 1st=15.08892822265625, 2nd=140.958, # queries below=270 fake x=-0.8787302602639084\n",
      "Total loss =  891.066\n",
      "#iter 107 x_co=340, 1st=-15.034440994262695, 2nd=107.689, # queries below=270 fake x=-0.5790906317071993\n",
      "Total loss =  890.328\n",
      "#iter 108 x_co=305, 1st=20.098960876464844, 2nd=147.196, # queries below=270 fake x=-0.7914093321362363\n",
      "Total loss =  889.185\n",
      "#iter 109 x_co=310, 1st=-15.000923156738281, 2nd=-16.5709, # queries below=270 fake x=-0.10880804735079641\n",
      "Total loss =  887.975\n",
      "#iter 110 x_co=2, 1st=27.09337615966797, 2nd=-18.3057, # queries below=270 fake x=1.2152874934015012\n",
      "Total loss =  890.100\n",
      "#iter 111 x_co=2, 1st=74.56362915039062, 2nd=192.943, # queries below=269 fake x=0.8288337778559589\n",
      "Total loss =  926.766\n",
      "#iter 112 x_co=289, 1st=31.461669921875, 2nd=154.235, # queries below=268 fake x=0.13436550240567757\n",
      "Total loss =  909.112\n",
      "#iter 113 x_co=13, 1st=20.887889862060547, 2nd=54.2693, # queries below=269 fake x=-0.347723484117308\n",
      "Total loss =  905.480\n",
      "#iter 114 x_co=132, 1st=-18.690345764160156, 2nd=34.5550, # queries below=269 fake x=1.5285337175349292\n",
      "Total loss =  901.139\n",
      "#iter 115 x_co=289, 1st=48.050811767578125, 2nd=188.818, # queries below=270 fake x=-0.12011639848116684\n",
      "Total loss =  900.132\n",
      "#iter 116 x_co=305, 1st=-18.553314208984375, 2nd=188.572, # queries below=270 fake x=-0.6930210013567366\n",
      "Total loss =  893.225\n",
      "#iter 117 x_co=310, 1st=-16.819129943847656, 2nd=-2.00626, # queries below=270 fake x=-8.492125609681173\n",
      "Total loss =  892.252\n",
      "#iter 118 x_co=310, 1st=-9006.943359375, 2nd=3594.08, # queries below=270 fake x=-5.986075003010049\n",
      "Total loss =  17655.7\n",
      "#iter 119 x_co=310, 1st=-2649.2294921875, 2nd=1609.14, # queries below=270 fake x=-4.339715915670061\n",
      "Total loss =  4089.06\n",
      "#iter 120 x_co=310, 1st=-771.710693359375, 2nd=727.487, # queries below=270 fake x=-3.2789263575921748\n",
      "Total loss =  1472.16\n",
      "#iter 121 x_co=310, 1st=-219.4940185546875, 2nd=336.829, # queries below=270 fake x=-2.6272780479277937\n",
      "Total loss =  983.058\n",
      "#iter 122 x_co=310, 1st=-58.57891845703125, 2nd=165.787, # queries below=271 fake x=-2.2739394326316553\n",
      "Total loss =  898.507\n",
      "#iter 123 x_co=393, 1st=14.668392181396484, 2nd=201.293, # queries below=271 fake x=-0.6935206427583851\n",
      "Total loss =  886.605\n",
      "#iter 124 x_co=103, 1st=14.260290145874023, 2nd=65.5592, # queries below=271 fake x=-0.17643102608093542\n",
      "Total loss =  886.095\n",
      "#iter 125 x_co=152, 1st=-13.946264266967773, 2nd=185.658, # queries below=271 fake x=-1.123727525558167\n",
      "Total loss =  884.374\n",
      "#iter 126 x_co=356, 1st=13.907020568847656, 2nd=52.8293, # queries below=271 fake x=-0.7537901577607566\n",
      "Total loss =  883.830\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m bool_coord_chosen \u001b[38;5;241m=\u001b[39m coordinate \u001b[38;5;129;01min\u001b[39;00m coords_chosen    \n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# if the coordinate is in the chosen pair of x_i, x_j\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bool_coord_chosen \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     coeff \u001b[38;5;241m=\u001b[39m df_sampled_queries[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoefficient\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39miloc[idx_q]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     83\u001b[0m     xi \u001b[38;5;241m=\u001b[39m df_sampled_queries[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpairs indices 1\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39miloc[idx_q]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#### COORDINATE DESCENT LOOP STARTS HERE ###\n",
    "#### In this loop, we do coordinate descent, NOT multivariate Newton's method ####\n",
    "\n",
    "# initialize number of coordinate descent iterations = 0\n",
    "num_iter_descent = 0\n",
    "q_idx = 0\n",
    "\n",
    "\n",
    "# while we don't have |q(X) - noisy_output|<lambda/2 for all q, continue coordinate descent \n",
    "while not np.all(error < Llambda/2):\n",
    "    # calculate total loss\n",
    "    total_loss = 0\n",
    "    q_idx = 0\n",
    "    while q_idx < k:\n",
    "        # calculate the loss of query indexed by q_idx\n",
    "        xi = sampled_queries_dict['pairs indices 1'][q_idx]\n",
    "        xi = fake_X[xi]\n",
    "        xj = sampled_queries_dict['pairs indices 2'][q_idx]\n",
    "        xj = fake_X[xj]\n",
    "        loss_q = loss_per_q.evalf(data_precision+4,subs={c: sampled_queries_dict['coefficient'][q_idx], x_i:xi, x_j:xj, q_hat:real_data_noisy_output[q_idx]})\n",
    "        total_loss += loss_q \n",
    "\n",
    "        # print(\"query #1 has loss = \", loss_q, 'total loss = ', total_loss)\n",
    "        q_idx += 1\n",
    "\n",
    "    # evaluate partial derivative wrt each coordinate of X\n",
    "    if num_iter_descent ==0: \n",
    "\n",
    "        # initialize the partial derivative of each coordinate to be zero\n",
    "        part_derivative = np.zeros(num_points)\n",
    "        \n",
    "        # compute the partial derivative of the loss function with respect to each coordinate     \n",
    "        for coordinate in range(num_points):\n",
    "            \n",
    "            # calculate partial derivative of total_loss wrt to the coordinate \n",
    "            # initiate the partial derivative of total loss to be 0.\n",
    "            part_deri_total_loss = 0 \n",
    "            \n",
    "            # eventually this is a sum over gradient_loss_per_q for all q in df_sampled_queries\n",
    "            for idx_q in range(k):\n",
    "                \n",
    "                # check if coordinate is one of the x_i x_j used in the query\n",
    "                coords_chosen = [sampled_queries_dict['pairs indices 1'][idx_q], sampled_queries_dict['pairs indices 2'][idx_q]]\n",
    "                bool_coord_chosen = coordinate in coords_chosen    \n",
    "                # if the coordinate is in the chosen pair of x_i, x_j\n",
    "                if bool_coord_chosen == True:\n",
    "                    coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "                    xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "                    xi = fake_X[xi]\n",
    "                    xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "                    xj = fake_X[xj]\n",
    "                    noisy_output = real_data_noisy_output[idx_q]\n",
    "                    part_deri_q_loss =  gradient_loss_per_q.evalf(data_precision+4, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "                \n",
    "                    # add the partial derivative of loss per every query to the total part deri\n",
    "                    part_deri_total_loss += part_deri_q_loss\n",
    "            \n",
    "            part_derivative[coordinate] = part_deri_total_loss\n",
    "            # print(coordinate, part_derivative[coordinate])\n",
    "        \n",
    "    else: \n",
    "    # change the partial derivative wrt to the queries which were impacted by x_coord_descent  \n",
    "        \n",
    "        idx_part_deri_to_change = df_sampled_queries[df_sampled_queries['pairs indices 1']==x_coord_descent]['pairs indices 2']\n",
    "        idx_part_deri_to_change = pd.concat([idx_part_deri_to_change,df_sampled_queries[df_sampled_queries['pairs indices 2']==x_coord_descent]['pairs indices 1']])\n",
    "        idx_part_deri_to_change = np.append(x_coord_descent, idx_part_deri_to_change.to_numpy())\n",
    "\n",
    "        for coordinate in idx_part_deri_to_change:\n",
    "            \n",
    "            # calculate partial derivative of total_loss wrt to the coordinate \n",
    "            # initiate the partial derivative of total loss to be 0.\n",
    "            part_deri_total_loss = 0 \n",
    "            \n",
    "            # eventually this is a sum over gradient_loss_per_q for all q in df_sampled_queries\n",
    "            for idx_q in range(k):\n",
    "                \n",
    "                # check if coordinate is one of the x_i x_j used in the query\n",
    "                coords_chosen =  [sampled_queries_dict['pairs indices 1'][idx_q], sampled_queries_dict['pairs indices 2'][idx_q]]\n",
    "                bool_coord_chosen = coordinate in coords_chosen    \n",
    "                # if the coordinate is in the chosen pair of x_i, x_j\n",
    "                if bool_coord_chosen == True:\n",
    "                    coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "                    xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "                    xi = fake_X[xi]\n",
    "                    xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "                    xj = fake_X[xj]\n",
    "                    noisy_output = real_data_noisy_output[idx_q]\n",
    "                    part_deri_q_loss =  gradient_loss_per_q.evalf(data_precision+4, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "                \n",
    "                    # add the partial derivative of loss per every query to the total part deri\n",
    "                    part_deri_total_loss += part_deri_q_loss\n",
    "            \n",
    "            part_derivative[coordinate] = part_deri_total_loss\n",
    "\n",
    "       \n",
    "    # find the coordinate with the max absolute value part_derivative \n",
    "    x_coord_descent = np.argmax(np.abs(part_derivative))\n",
    "     \n",
    "    # update x value at the coordinate x_coord_descent \n",
    "    \n",
    "    # First, we calculate the 2nd derivative wrt x_coord_descent\n",
    "    deg_2_part_deri_total_loss = 0\n",
    "    for idx_q in range(k):\n",
    "            \n",
    "        # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "        coords_chosen =  [sampled_queries_dict['pairs indices 1'][idx_q], sampled_queries_dict['pairs indices 2'][idx_q]]\n",
    "        bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "        # if the x_coord_descent is in the chosen pair of x_i, x_j\n",
    "        if bool_coord_chosen == True:\n",
    "            coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "            xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "            xi = fake_X[xi]\n",
    "            xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "            xj = fake_X[xj]\n",
    "            noisy_output = real_data_noisy_output[idx_q]\n",
    "            deg_2_part_deri_q_loss =  hession_loss_per_q.evalf(data_precision+4, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "        \n",
    "            # add the partial derivative of loss per every query to the total part deri\n",
    "            deg_2_part_deri_total_loss += deg_2_part_deri_q_loss\n",
    "    \n",
    "    # Now we are ready to update fake_X at x_coord_descent\n",
    "    fake_X[x_coord_descent] = fake_X[x_coord_descent] - (part_derivative[x_coord_descent]/deg_2_part_deri_total_loss)\n",
    "    \n",
    "    \n",
    "    # recalculate error for queries that were impacted\n",
    "    # save a list of queries whose error were updated bc of change in x_coord_descent\n",
    "    which_q_impacted = [] \n",
    "    for idx_q in range(k):\n",
    "            \n",
    "        # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "        coords_chosen =  [sampled_queries_dict['pairs indices 1'][idx_q], sampled_queries_dict['pairs indices 2'][idx_q]]\n",
    "        bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "        # if the coordinate is in the chosen pair of x_i, x_j, we update the error for that query\n",
    "        if bool_coord_chosen == True:\n",
    "        \n",
    "            # compute query output on the current synopsis \n",
    "            fake_xi = fake_X[df_sampled_queries['pairs indices 1'].iloc[idx_q]]\n",
    "            fake_xj = fake_X[df_sampled_queries['pairs indices 2'].iloc[idx_q]]\n",
    "    \n",
    "            # compute query output on fake data \n",
    "            fake_data_output[idx_q] = deg_2_poly.evalf(data_precision+4, subs={c:df_sampled_queries['coefficient'].iloc[idx_q], x_i:fake_xi, x_j:fake_xj})\n",
    " \n",
    "            # update error for that query\n",
    "            # notice that this is |q(X) - real_data_noisy_output|\n",
    "            error[idx_q] = abs(fake_data_output[idx_q]-real_data_noisy_output[idx_q])\n",
    "            which_q_impacted.append(idx_q)\n",
    "    \n",
    "    \n",
    "    print(f\"#iter {num_iter_descent} x_co={x_coord_descent}, 1st={part_derivative[x_coord_descent]}, 2nd={ deg_2_part_deri_total_loss}, # queries below={sum(error<Llambda)} fake x={fake_X[x_coord_descent]}\")\n",
    "    print('Total loss = ', total_loss)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # add 1 to num_iter_descent \n",
    "    num_iter_descent += 1\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eccf48",
   "metadata": {},
   "source": [
    "1. plot loss function and see if it's decreasing. If not, that means we have a bug \n",
    "2. try fake_X = real_X and see if it converges quickly. if not, might mean we have added too much noise\n",
    "3. if our current code's loss function is converging correctly, and it just needs more time, we can let it run for longer and speed up the code by descending on all coordinates at once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6738f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_queries_dict = df_sampled_queries.reset_index().to_dict()\n",
    "\n",
    "\n",
    "# for idx_q in range(k):\n",
    "    \n",
    "# #     # check if coordinate is one of the x_i x_j used in the query\n",
    "#     coords_chosen = [sampled_queries_dict['pairs indices 1'][idx_q], sampled_queries_dict['pairs indices 2'][idx_q]]\n",
    "\n",
    "# coords_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ccfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #### COORDINATE DESCENT LOOP STARTS HERE ###\n",
    "# #### In this loop, we do coordinate descent, NOT multivariate Newton's method ####\n",
    "\n",
    "# # initialize number of coordinate descent iterations = 0\n",
    "# num_iter_descent = 0\n",
    "\n",
    "# # while we don't have |q(X) - noisy_output|<lambda/2 for all q, continue coordinate descent \n",
    "# while not np.all(error < Llambda/2):\n",
    "    \n",
    "#     # initialize the partial derivative of each coordinate to be zero\n",
    "#     part_derivative = np.zeros(num_points)\n",
    "    \n",
    "#     # compute the partial derivative of the loss function with respect to each coordinate     \n",
    "#     for coordinate in range(num_points):\n",
    "        \n",
    "#         # calculate partial derivative of total_loss wrt to the coordinate \n",
    "#         # initiate the partial derivative of total loss to be 0.\n",
    "#         part_deri_total_loss = 0 \n",
    "        \n",
    "#         # eventually this is a sum over gradient_loss_per_q for all q in df_sampled_queries\n",
    "#         for idx_q in range(k):\n",
    "            \n",
    "#             # check if coordinate is one of the x_i x_j used in the query\n",
    "#             coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#             bool_coord_chosen = coordinate in coords_chosen    \n",
    "#             # if the coordinate is in the chosen pair of x_i, x_j\n",
    "#             if bool_coord_chosen == True:\n",
    "#                 coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "#                 xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "#                 xi = fake_X[xi]\n",
    "#                 xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "#                 xj = fake_X[xj]\n",
    "#                 noisy_output = real_data_noisy_output[idx_q]\n",
    "#                 part_deri_q_loss =  gradient_loss_per_q.evalf(data_precision, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "            \n",
    "#             # if the coordinate is not one of the chosen ones, then the partial derivative = 0\n",
    "#             else: \n",
    "                \n",
    "#                 part_deri_q_loss = 0\n",
    "            \n",
    "#             # add the partial derivative of loss per every query to the total part deri\n",
    "#             part_deri_total_loss += part_deri_q_loss\n",
    "        \n",
    "#         part_derivative[coordinate] = part_deri_total_loss\n",
    "        \n",
    "#     part_deri_copy_1 = part_derivative \n",
    "       \n",
    "#     # find the coordinate with a negative part_derivative which has the largest absolute value\n",
    "    \n",
    "#     # Get indices of negative values\n",
    "#     negative_part_deri_indices = np.where(part_derivative < 0)[0]    \n",
    "#     # Get the negative values\n",
    "#     negative_part_deri_values = part_derivative[negative_part_deri_indices]    \n",
    "#     # Find the index of the maximum absolute value among the negative values\n",
    "#     max_neg_idx = np.argmax(np.abs(negative_part_deri_values))\n",
    "#     # Get the index in part_derivative\n",
    "#     x_coord_descent = negative_part_deri_indices[max_neg_idx]\n",
    "#     #     # Get the value from part_derivative\n",
    "#     # max_part_deri_value = part_derivative[max_part_deri_index]\n",
    "    \n",
    "    \n",
    "#     # update x value at the coordinate x_coord_descent \n",
    "    \n",
    "#     # First, we calculate the 2nd derivative wrt x_coord_descent\n",
    "#     deg_2_part_deri_total_loss = 0\n",
    "#     for idx_q in range(k):\n",
    "            \n",
    "#             # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "#             coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#             bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "#             # if the x_coord_descent is in the chosen pair of x_i, x_j\n",
    "#             if bool_coord_chosen == True:\n",
    "#                 coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "#                 xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "#                 xi = fake_X[xi]\n",
    "#                 xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "#                 xj = fake_X[xj]\n",
    "#                 noisy_output = real_data_noisy_output[idx_q]\n",
    "#                 deg_2_part_deri_q_loss =  hession_loss_per_q.evalf(data_precision, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "            \n",
    "#             # if the coordinate is not one of the chosen ones, then the partial derivative = 0\n",
    "#             else: \n",
    "                \n",
    "#                 deg_2_part_deri_q_loss = 0\n",
    "            \n",
    "#             # add the partial derivative of loss per every query to the total part deri\n",
    "#             deg_2_part_deri_total_loss += deg_2_part_deri_q_loss\n",
    "      \n",
    "#     # Now we are ready to update fake_X at x_coord_descent\n",
    "#     fake_X[x_coord_descent] = fake_X[x_coord_descent] - (part_derivative[x_coord_descent]/deg_2_part_deri_total_loss)\n",
    "    \n",
    "    \n",
    "#     # recalculate error for queries that were impacted\n",
    "#     # save a list of queries whose error were updated bc of change in x_coord_descent\n",
    "#     which_q_impacted = [] \n",
    "#     for idx_q in range(k):\n",
    "            \n",
    "#         # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "#         coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#         bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "#         # if the coordinate is in the chosen pair of x_i, x_j, we update the error for that query\n",
    "#         if bool_coord_chosen == True:\n",
    "        \n",
    "#             # compute query output on the current synopsis \n",
    "#             fake_xi = fake_X[df_sampled_queries['pairs indices 1'].iloc[idx_q]]\n",
    "#             fake_xj = fake_X[df_sampled_queries['pairs indices 2'].iloc[idx_q]]\n",
    "    \n",
    "#             # compute query output on fake data \n",
    "#             fake_data_output[idx_q] = deg_2_poly.evalf(data_precision, subs={c:df_sampled_queries['coefficient'].iloc[idx_q], x_i:fake_xi, x_j:fake_xj})\n",
    " \n",
    "#             # update error for that query\n",
    "#             # notice that this is |q(X) - real_data_noisy_output|\n",
    "#             error[idx_q] = abs(fake_data_output[idx_q]-real_data_noisy_output[idx_q])\n",
    "#             which_q_impacted.append(idx_q)\n",
    "    \n",
    "#     # add 1 to num_iter_descent \n",
    "#     num_iter_descent += 1\n",
    "    \n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251da74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### DEBUGGING CELL 1 \n",
    "# # Time = 2 ins\n",
    "# # initialize the partial derivative of each coordinate to be zero\n",
    "# part_derivative = np.zeros(num_points)\n",
    "\n",
    "# num_iter_chosen = 0\n",
    "\n",
    "\n",
    "\n",
    "# # compute the partial derivative of the loss function with respect to each coordinate     \n",
    "# for coordinate in range(num_points):\n",
    "    \n",
    "#     # calculate partial derivative of total_loss wrt to the coordinate \n",
    "#     # initiate the partial derivative of total loss to be 0.\n",
    "#     part_deri_total_loss = 0 \n",
    "    \n",
    "#     # eventually this is a sum over gradient_loss_per_q for all q in df_sampled_queries\n",
    "#     for idx_q in range(k):\n",
    "        \n",
    "#         # check if coordinate is one of the x_i x_j used in the query\n",
    "#         # The next two lines take 0.0005 s to run\n",
    "#         # start_time = time.time()\n",
    "#         coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#         bool_coord_chosen = coordinate in coords_chosen    \n",
    "#         # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "\n",
    "#         # if the coordinate is in the chosen pair of x_i, x_j\n",
    "#         if bool_coord_chosen == True: # time = 0.002 - 0.003\n",
    "#             num_iter_chosen += 1\n",
    "#             coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0] # time = 0.0003 \n",
    "#             xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "#             xi = fake_X[xi]\n",
    "#             xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "#             xj = fake_X[xj]\n",
    "            \n",
    "#             noisy_output = real_data_noisy_output[idx_q] # time = 10^-7\n",
    "            \n",
    "#             # start_time = time.time()\n",
    "#             part_deri_q_loss =  gradient_loss_per_q.evalf(data_precision, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output}) # time = 0.0015 - 0.002\n",
    "#             # print(coordinate, idx_q, \"--- %s seconds ---\" % (time.time() - start_time))\n",
    "           \n",
    "#         # if the coordinate is not one of the chosen ones, then the partial derivative = 0\n",
    "            \n",
    "#         else: \n",
    "#             # start_time = time.time()\n",
    "#             part_deri_q_loss = 0\n",
    "#             # print(coordinate, idx_q, \"--- %s seconds ---\" % (time.time() - start_time))\n",
    "#         # add the partial derivative of loss per every query to the total part deri\n",
    "#         part_deri_total_loss += part_deri_q_loss\n",
    "        \n",
    "    \n",
    "#     part_derivative[coordinate] = part_deri_total_loss\n",
    "\n",
    "# # print(num_iter_chosen)   \n",
    "    \n",
    "# # find the coordinate with a negative part_derivative which has the largest absolute value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114da6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DEBUGGING CELL 2\n",
    "\n",
    "# # Get indices of negative values\n",
    "# # Time = 0.0003 \n",
    "# negative_part_deri_indices = np.where(part_derivative < 0)[0]    \n",
    "# # Get the negative values\n",
    "# negative_part_deri_values = part_derivative[negative_part_deri_indices]    \n",
    "# # Find the index of the maximum absolute value among the negative values\n",
    "# max_neg_idx = np.argmax(np.abs(negative_part_deri_values))\n",
    "# # Get the index in part_derivative\n",
    "# x_coord_descent = negative_part_deri_indices[max_neg_idx]\n",
    "# #     # Get the value from part_derivative\n",
    "# # max_part_deri_value = part_derivative[max_part_deri_index]\n",
    "\n",
    "\n",
    "# # update x value at the coordinate x_coord_descent \n",
    "\n",
    "# # First, we calculate the 2nd derivative wrt x_coord_descent\n",
    "# deg_2_part_deri_total_loss = 0\n",
    "\n",
    "# for idx_q in range(k): # Time = ~0.9 \n",
    "        \n",
    "#         # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "#         coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#         bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "#         # if the x_coord_descent is in the chosen pair of x_i, x_j\n",
    "#         if bool_coord_chosen == True: # Time = 0.01 \n",
    "            \n",
    "#             coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "#             xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "#             xi = fake_X[xi]\n",
    "#             xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "#             xj = fake_X[xj]\n",
    "#             noisy_output = real_data_noisy_output[idx_q]\n",
    "#             deg_2_part_deri_q_loss =  hession_loss_per_q.evalf(data_precision, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "        \n",
    "            \n",
    "#         # if the coordinate is not one of the chosen ones, then the partial derivative = 0\n",
    "#         else: \n",
    "            \n",
    "#             deg_2_part_deri_q_loss = 0\n",
    "        \n",
    "#         # add the partial derivative of loss per every query to the total part deri\n",
    "#         deg_2_part_deri_total_loss += deg_2_part_deri_q_loss\n",
    "\n",
    "# # Now we are ready to update fake_X at x_coord_descent\n",
    "# # start_time = time.time()\n",
    "# # Time = 0.002 \n",
    "# fake_X[x_coord_descent] = fake_X[x_coord_descent] - (part_derivative[x_coord_descent]/deg_2_part_deri_total_loss)\n",
    "# # print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# # recalculate error for queries that were impacted\n",
    "# # save a list of queries whose error were updated bc of change in x_coord_descent\n",
    "# which_q_impacted = [] \n",
    "# for idx_q in range(k):\n",
    "        \n",
    "#     # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "#     coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#     bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "#     # if the coordinate is in the chosen pair of x_i, x_j, we update the error for that query\n",
    "    \n",
    "#     if bool_coord_chosen == True: # Time = 0.02 \n",
    "        \n",
    "#         # compute query input on the current synopsis \n",
    "#         fake_xi = fake_X[df_sampled_queries['pairs indices 1'].iloc[idx_q]]\n",
    "#         fake_xj = fake_X[df_sampled_queries['pairs indices 2'].iloc[idx_q]]\n",
    "\n",
    "#         # compute query output on fake data \n",
    "#         fake_data_output[idx_q] = deg_2_poly.evalf(data_precision, subs={c:df_sampled_queries['coefficient'].iloc[idx_q], x_i:fake_xi, x_j:fake_xj})\n",
    "\n",
    "#         # update error for that query\n",
    "#         # notice that this is |q(X) - real_data_noisy_output|\n",
    "#         error[idx_q] = abs(fake_data_output[idx_q]-real_data_noisy_output[idx_q])\n",
    "#         which_q_impacted.append(idx_q)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #### COORDINATE DESCENT LOOP STARTS HERE ###\n",
    "# #### In this loop, we do coordinate descent, NOT multivariate Newton's method ####\n",
    "\n",
    "# # initialize number of coordinate descent iterations = 0\n",
    "# num_iter_descent = 0\n",
    "\n",
    "# # while we don't have |q(X) - noisy_output|<lambda/2 for all q, continue coordinate descent \n",
    "# while not np.all(error < Llambda/2):\n",
    "    \n",
    "#     # initialize the partial derivative of each coordinate to be zero\n",
    "#     part_derivative = np.zeros(num_points)\n",
    "    \n",
    "#     # compute the partial derivative of the loss function with respect to each coordinate     \n",
    "#     for coordinate in range(num_points):\n",
    "        \n",
    "#         # calculate partial derivative of total_loss wrt to the coordinate \n",
    "#         # initiate the partial derivative of total loss to be 0.\n",
    "#         part_deri_total_loss = 0 \n",
    "        \n",
    "#         # eventually this is a sum over gradient_loss_per_q for all q in df_sampled_queries\n",
    "#         for idx_q in range(k):\n",
    "            \n",
    "#             # check if coordinate is one of the x_i x_j used in the query\n",
    "#             coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#             bool_coord_chosen = coordinate in coords_chosen    \n",
    "#             # if the coordinate is in the chosen pair of x_i, x_j\n",
    "#             if bool_coord_chosen == True:\n",
    "#                 coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "#                 xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "#                 xi = fake_X[xi]\n",
    "#                 xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "#                 xj = fake_X[xj]\n",
    "#                 noisy_output = real_data_noisy_output[idx_q]\n",
    "#                 part_deri_q_loss =  gradient_loss_per_q.evalf(data_precision, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "            \n",
    "#             # if the coordinate is not one of the chosen ones, then the partial derivative = 0\n",
    "#             else: \n",
    "                \n",
    "#                 part_deri_q_loss = 0\n",
    "            \n",
    "#             # add the partial derivative of loss per every query to the total part deri\n",
    "#             part_deri_total_loss += part_deri_q_loss\n",
    "        \n",
    "#         part_derivative[coordinate] = part_deri_total_loss\n",
    "        \n",
    "        \n",
    "#     # find the coordinate with a negative part_derivative which has the largest absolute value\n",
    "    \n",
    "#     # Get indices of negative values\n",
    "#     negative_part_deri_indices = np.where(part_derivative < 0)[0]    \n",
    "#     # Get the negative values\n",
    "#     negative_part_deri_values = part_derivative[negative_part_deri_indices]    \n",
    "#     # Find the index of the maximum absolute value among the negative values\n",
    "#     max_neg_idx = np.argmax(np.abs(negative_part_deri_values))\n",
    "#     # Get the index in part_derivative\n",
    "#     x_coord_descent = negative_part_deri_indices[max_neg_idx]\n",
    "#     #     # Get the value from part_derivative\n",
    "#     # max_part_deri_value = part_derivative[max_part_deri_index]\n",
    "    \n",
    "    \n",
    "#     # update x value at the coordinate x_coord_descent \n",
    "    \n",
    "#     # First, we calculate the 2nd derivative wrt x_coord_descent\n",
    "#     deg_2_part_deri_total_loss = 0\n",
    "#     for idx_q in range(k):\n",
    "            \n",
    "#             # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "#             coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#             bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "#             # if the x_coord_descent is in the chosen pair of x_i, x_j\n",
    "#             if bool_coord_chosen == True:\n",
    "#                 coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "#                 xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "#                 xi = fake_X[xi]\n",
    "#                 xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "#                 xj = fake_X[xj]\n",
    "#                 noisy_output = real_data_noisy_output[idx_q]\n",
    "#                 deg_2_part_deri_q_loss =  hession_loss_per_q.evalf(data_precision, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "            \n",
    "#             # if the coordinate is not one of the chosen ones, then the partial derivative = 0\n",
    "#             else: \n",
    "                \n",
    "#                 deg_2_part_deri_q_loss = 0\n",
    "            \n",
    "#             # add the partial derivative of loss per every query to the total part deri\n",
    "#             deg_2_part_deri_total_loss += deg_2_part_deri_q_loss\n",
    "      \n",
    "#     # Now we are ready to update fake_X at x_coord_descent\n",
    "#     fake_X[x_coord_descent] = fake_X[x_coord_descent] - (part_derivative[x_coord_descent]/deg_2_part_deri_total_loss)\n",
    "    \n",
    "    \n",
    "#     # recalculate error for queries that were impacted\n",
    "#     # save a list of queries whose error were updated bc of change in x_coord_descent\n",
    "#     which_q_impacted = [] \n",
    "#     for idx_q in range(k):\n",
    "            \n",
    "#         # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "#         coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "#         bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "#         # if the coordinate is in the chosen pair of x_i, x_j, we update the error for that query\n",
    "#         if bool_coord_chosen == True:\n",
    "        \n",
    "#             # compute query output on the current synopsis \n",
    "#             fake_xi = fake_X[df_sampled_queries['pairs indices 1'].iloc[idx_q]]\n",
    "#             fake_xj = fake_X[df_sampled_queries['pairs indices 2'].iloc[idx_q]]\n",
    "    \n",
    "#             # compute query output on fake data \n",
    "#             fake_data_output[idx_q] = deg_2_poly.evalf(data_precision, subs={c:df_sampled_queries['coefficient'].iloc[idx_q], x_i:fake_xi, x_j:fake_xj})\n",
    " \n",
    "#             # update error for that query\n",
    "#             # notice that this is |q(X) - real_data_noisy_output|\n",
    "#             error[idx_q] = abs(fake_data_output[idx_q]-real_data_noisy_output[idx_q])\n",
    "#             which_q_impacted.append(idx_q)\n",
    "    \n",
    "#     # add 1 to num_iter_descent \n",
    "#     num_iter_descent += 1\n",
    "    \n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7442eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query does the following:\n",
    "# Given a database X, it selects two points x_i, x_j \\in X uniformly at random and computes\n",
    "# q_c(X) = c(x_i + x_j)^2.\n",
    "# The set of queries Q consists of all such q_c with coefficient \\in (0, 1] \n",
    "# Note that we are not setting the precision of coefficients to allow a large size of Q\n",
    "\n",
    "# c, x_i, x_j = sp.symbols('c x_i x_j')\n",
    "\n",
    "# # deg_2_poly = Function('deg_2_poly')\n",
    "\n",
    "# deg_2_poly = c*(x_i+x_j)**2\n",
    "\n",
    "# gradient_deg_2_poly = sp.diff(deg_2_poly, x_i)\n",
    "\n",
    "# # evaluate a function\n",
    "# gradient_deg_2_poly.evalf(4, subs={c:1, x_i:2.32, x_j:3})\n",
    "\n",
    "# gradient_deg_2_poly(1,2,3)\n",
    "\n",
    "# def query_quadratic(data, coefficient):\n",
    "    \n",
    "#     # select a pair of x_i and x_j uniformly at random from data\n",
    "#     pair_indices = np.random.choice(len(data) , size = 2, replace = False, p = None)\n",
    "#     pair = data[pair_indices]\n",
    "#     output = coefficient*(pair[0]+pair[1])**2\n",
    "    \n",
    "#     return pair_indices, pair, output\n",
    "\n",
    "# partial derivative of the loss function wrt to a selected coordinate of X\n",
    "# this function is specific for the query q(x) = c(x_i+x_j)^2\n",
    "# we might be able to generalize this using built in derivative functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def partial_derivative_loss(X, coordinate, c):\n",
    "#     part_deri_total_loss = 0\n",
    "#     order_2_part_deri_total_loss = 0\n",
    "#     for index, coefficient in enumerate(c):\n",
    "#         # check if the coordinate wrt which we are taking the partial derivative is in the pair\n",
    "#         # if in the pair, take the partial derivative, otherwise partial derivative = 0\n",
    "#         if coordinate in pairs_indices[index,:]:\n",
    "#             x_i = pairs[index,:][0]\n",
    "#             x_j = pairs[index,:][0]\n",
    "#             part_deri_query_loss = (1/Llambda**2)*coefficient**2*4*(x_i+x_j)**3-4*noisy_output[index]*coefficient*(x_i+x_j)\n",
    "#             order_2_part_deri_query_loss = (1/Llambda**2)*12*coefficient**2*(x_i+x_j)**2-4*coefficient*noisy_output[index]\n",
    "#         else: \n",
    "#             part_deri_query_loss = 0\n",
    "#             order_2_part_deri_query_loss = 0\n",
    "#         part_deri_total_loss += part_deri_query_loss\n",
    "#         order_2_part_deri_total_loss += order_2_part_deri_query_loss\n",
    "#     return part_deri_total_loss, order_2_part_deri_total_loss\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e1e40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates a (k, lambda, eta, beta)- base synopsis generator that is (epsilon,delta)-DP \n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import sympy as sp\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "lower_bound = -1 # data lower bound\n",
    "upper_bound = 1  # data upper bound\n",
    "data_precision = 2 # data precision\n",
    "\n",
    "n = 200 \n",
    "num_points = 2*n # number of data points\n",
    "m = (upper_bound - lower_bound)/10**(-data_precision) +1 # size of the domain universe\n",
    "\n",
    "delta =  0.02 # DP parameter\n",
    "eta = 0.025 # edge for boosting\n",
    "beta = 0.1 # failure probability of the base synopsis\n",
    "k = math.ceil(2*((math.log(2/beta)+m)/(1-2*eta))) # number of query sample as demanded by Lemma 6.5 \n",
    "# Assume coefficient \\in (0,1], changing one x_i can at most change 1*[(1+x_j)^2 - (-1+x_j)^2] = 4x_j <= 4.\n",
    "rho = 4 # l_1 sensitivity of our query = 4\n",
    "\n",
    "Llambda = 0.2 # accuracy parameter lambda\n",
    "epsilon = (math.log(1/beta)*rho*math.sqrt(k*math.log(1/delta)))/Llambda\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aee1594a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8261b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query does the following:\n",
    "# Given a database X, it selects two points x_i, x_j \\in X uniformly at random and computes\n",
    "# q_c(X) = c(x_i + x_j)^2.\n",
    "# The set of queries Q consists of all such q_c with coefficient c \\in (0, 1] \n",
    "# Note that we are not setting the precision of coefficients to allow a large size of Q\n",
    "\n",
    "c, x_i, x_j = sp.symbols('c x_i x_j')\n",
    "\n",
    "# deg_2_poly = Function('deg_2_poly')\n",
    "\n",
    "deg_2_poly = c*(x_i+x_j)**2\n",
    "\n",
    "gradient_deg_2_poly = sp.diff(deg_2_poly, x_i)\n",
    "hession_deg_2_poly = sp.diff(gradient_deg_2_poly, x_i)\n",
    "\n",
    "q_hat = sp.symbols('q_hat')\n",
    "\n",
    "gradient_loss_per_q = (1/Llambda**2)*2*(deg_2_poly-q_hat)*gradient_deg_2_poly\n",
    "hession_loss_per_q = (1/Llambda**2)*2*(gradient_deg_2_poly)**2+2*(deg_2_poly-q_hat)*hession_deg_2_poly\n",
    "\n",
    "# evaluate a function\n",
    "# deg_2_poly.evalf(4, subs={c:1, x_i:2.32, x_j:3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d61310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all possible pairs of data among the given data points\n",
    "all_possible_pairs= np.array(list(combinations(range(num_points), 2)))\n",
    "\n",
    "# The entire query set Q corresponds to a coefficient_array of (0,1] of precision 0.001\n",
    "all_coeff = 1-np.linspace(0, 1, num=len(all_possible_pairs), endpoint=False, dtype=None)\n",
    "\n",
    "# for each query, we also need to fix a pair of x_i, x_j\n",
    "UAR_among_all_pairs= np.random.choice(range(len(all_possible_pairs)), size=len(all_coeff), replace=False)\n",
    "\n",
    "# the k-th row stores a pair of indices (i,j) used by q_k = c_k(x_i+x_j)^2\n",
    "all_pairs_indices = all_possible_pairs[UAR_among_all_pairs,:] \n",
    "\n",
    "# create a dataframe to store the coefficient and pairs indices for each query in Q\n",
    "df_deg_2_poly = {'coefficient': all_coeff, 'pairs indices 1': all_pairs_indices[:,0],'pairs indices 2': all_pairs_indices[:,1]}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_deg_2_poly = pd.DataFrame(df_deg_2_poly).apply(lambda x: round(x, data_precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e65282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide on a set of real data \n",
    "real_X = np.random.uniform(low=lower_bound, high=upper_bound, size=num_points)\n",
    "# initialize the synopsis to be some arbirary set of data, say from the standard normal\n",
    "fake_X = np.random.randn(num_points)\n",
    "fake_X_copy = fake_X # save a copy of fake_X\n",
    "\n",
    "# for verification purposes, if fake_X = real_X, the initial error should be the same as the added laplace noise\n",
    "# fake_X = real_X \n",
    "\n",
    "# To initialize boosting, we choose UAR a size k subset of Q \n",
    "# initially, queries are sampled UAR. This distribution will change after each iteration of boosting\n",
    "df_sampled_queries = df_deg_2_poly.sample(n=k, replace=False, weights=None, axis=None)\n",
    "\n",
    "#### BOOSTING LOOP STARTS ####\n",
    "\n",
    "# initialize all-zero arrays to store noiselss query output, noisy output, and laplace noise  \n",
    "real_output = np.zeros(k)\n",
    "real_data_noisy_output = np.zeros(k)\n",
    "lap_noise = np.zeros(k)\n",
    "fake_data_output = np.zeros(k)\n",
    "error = np.zeros(k) # store |q(X) - noisy_output| for each q\n",
    "\n",
    "\n",
    "# for each query, compute its real output, noisy output, and error\n",
    "for index, coefficient in enumerate(df_sampled_queries['coefficient']):\n",
    "    # store the original index \n",
    "    ori_idx_queries = df_sampled_queries['coefficient'].index\n",
    "    # store the real x_i, x_j used for each query\n",
    "    real_xi = real_X[df_sampled_queries['pairs indices 1'][ori_idx_queries[index]]]\n",
    "    real_xj = real_X[df_sampled_queries['pairs indices 2'][ori_idx_queries[index]]]\n",
    "    # real output\n",
    "    real_output[index] = deg_2_poly.evalf(data_precision, subs={c:df_sampled_queries['coefficient'][ori_idx_queries[index]], x_i:real_xi, x_j:real_xj})\n",
    "\n",
    "    \n",
    "    # compute noisy output on the real data\n",
    "    lap_noise[index] = np.random.laplace(loc=0.0, scale=rho*(2*math.sqrt(2*k*math.log(1/delta))/epsilon), size=None) \n",
    "    real_data_noisy_output[index] = real_output[index] + lap_noise[index]\n",
    "    \n",
    "    # compute query output on the current synopsis \n",
    "    fake_xi = fake_X[df_sampled_queries['pairs indices 1'][ori_idx_queries[index]]]\n",
    "    fake_xj = fake_X[df_sampled_queries['pairs indices 2'][ori_idx_queries[index]]]\n",
    "    \n",
    "    # compute query output on fake data \n",
    "    fake_data_output[index] = deg_2_poly.evalf(data_precision, subs={c:df_sampled_queries['coefficient'][ori_idx_queries[index]], x_i:fake_xi, x_j:fake_xj})\n",
    " \n",
    "    # calculate initial error\n",
    "    # notice that this is |q(X) - real_data_noisy_output|\n",
    "    error[index] = abs(fake_data_output[index]-real_data_noisy_output[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "251da74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114da6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "618e413e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# eventually this is a sum over gradient_loss_per_q for all q in df_sampled_queries\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx_q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[1;32m     22\u001b[0m     \n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# check if coordinate is one of the x_i x_j used in the query\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     coords_chosen \u001b[38;5;241m=\u001b[39m [\u001b[43mdf_sampled_queries\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpairs indices 1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpairs indices 2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[idx_q]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], df_sampled_queries[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpairs indices 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpairs indices 2\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39miloc[idx_q]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m     25\u001b[0m     bool_coord_chosen \u001b[38;5;241m=\u001b[39m coordinate \u001b[38;5;129;01min\u001b[39;00m coords_chosen    \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# if the coordinate is in the chosen pair of x_i, x_j\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:6110\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6107\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   6109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6110\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6111\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex(keyarr)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:6097\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6079\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6080\u001b[0m \u001b[38;5;124;03mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   6081\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6094\u001b[0m \u001b[38;5;124;03marray([0, 2])\u001b[39;00m\n\u001b[1;32m   6095\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6097\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6098\u001b[0m indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   6099\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:3870\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3868\u001b[0m method \u001b[38;5;241m=\u001b[39m clean_reindex_fill_method(method)\n\u001b[1;32m   3869\u001b[0m orig_target \u001b[38;5;241m=\u001b[39m target\n\u001b[0;32m-> 3870\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_cast_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3872\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:6623\u001b[0m, in \u001b[0;36mIndex._maybe_cast_listlike_indexer\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_cast_listlike_indexer\u001b[39m(\u001b[38;5;28mself\u001b[39m, target) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m   6620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6621\u001b[0m \u001b[38;5;124;03m    Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\u001b[39;00m\n\u001b[1;32m   6622\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:7577\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7575\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/base.py:560\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    557\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 560\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be specified when data is not list-like\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/construction.py:608\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    606\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 608\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    610\u001b[0m         object_index\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[1;32m    613\u001b[0m     ):\n\u001b[1;32m    614\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/dtypes/cast.py:1180\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[0;32m-> 1180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;49;00m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#  numpy would have done it for us.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_non_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_if_all_nat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mM8[ns]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#### COORDINATE DESCENT LOOP STARTS HERE ###\n",
    "#### In this loop, we do coordinate descent, NOT multivariate Newton's method ####\n",
    "\n",
    "# initialize number of coordinate descent iterations = 0\n",
    "num_iter_descent = 0\n",
    "\n",
    "# while we don't have |q(X) - noisy_output|<lambda/2 for all q, continue coordinate descent \n",
    "while not np.all(error < Llambda/2):\n",
    "    \n",
    "    # initialize the partial derivative of each coordinate to be zero\n",
    "    part_derivative = np.zeros(num_points)\n",
    "    \n",
    "    # compute the partial derivative of the loss function with respect to each coordinate     \n",
    "    for coordinate in range(num_points):\n",
    "        \n",
    "        # calculate partial derivative of total_loss wrt to the coordinate \n",
    "        # initiate the partial derivative of total loss to be 0.\n",
    "        part_deri_total_loss = 0 \n",
    "        \n",
    "        # eventually this is a sum over gradient_loss_per_q for all q in df_sampled_queries\n",
    "        for idx_q in range(k):\n",
    "            \n",
    "            # check if coordinate is one of the x_i x_j used in the query\n",
    "            coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "            bool_coord_chosen = coordinate in coords_chosen    \n",
    "            # if the coordinate is in the chosen pair of x_i, x_j\n",
    "            if bool_coord_chosen == True:\n",
    "                coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "                xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "                xi = fake_X[xi]\n",
    "                xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "                xj = fake_X[xj]\n",
    "                noisy_output = real_data_noisy_output[idx_q]\n",
    "                part_deri_q_loss =  gradient_loss_per_q.evalf(data_precision, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "            \n",
    "            # if the coordinate is not one of the chosen ones, then the partial derivative = 0\n",
    "            else: \n",
    "                \n",
    "                part_deri_q_loss = 0\n",
    "            \n",
    "            # add the partial derivative of loss per every query to the total part deri\n",
    "            part_deri_total_loss += part_deri_q_loss\n",
    "        \n",
    "        part_derivative[coordinate] = part_deri_total_loss\n",
    "        \n",
    "        \n",
    "    # find the coordinate with a negative part_derivative which has the largest absolute value\n",
    "    \n",
    "    # Get indices of negative values\n",
    "    negative_part_deri_indices = np.where(part_derivative < 0)[0]    \n",
    "    # Get the negative values\n",
    "    negative_part_deri_values = part_derivative[negative_part_deri_indices]    \n",
    "    # Find the index of the maximum absolute value among the negative values\n",
    "    max_neg_idx = np.argmax(np.abs(negative_part_deri_values))\n",
    "    # Get the index in part_derivative\n",
    "    x_coord_descent = negative_part_deri_indices[max_neg_idx]\n",
    "    #     # Get the value from part_derivative\n",
    "    # max_part_deri_value = part_derivative[max_part_deri_index]\n",
    "    \n",
    "    \n",
    "    # update x value at the coordinate x_coord_descent \n",
    "    \n",
    "    # First, we calculate the 2nd derivative wrt x_coord_descent\n",
    "    deg_2_part_deri_total_loss = 0\n",
    "    for idx_q in range(k):\n",
    "            \n",
    "            # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "            coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "            bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "            # if the x_coord_descent is in the chosen pair of x_i, x_j\n",
    "            if bool_coord_chosen == True:\n",
    "                coeff = df_sampled_queries[['coefficient']].iloc[idx_q].iloc[0]\n",
    "                xi = df_sampled_queries[['pairs indices 1']].iloc[idx_q].iloc[0]\n",
    "                xi = fake_X[xi]\n",
    "                xj = df_sampled_queries[['pairs indices 2']].iloc[idx_q].iloc[0]\n",
    "                xj = fake_X[xj]\n",
    "                noisy_output = real_data_noisy_output[idx_q]\n",
    "                deg_2_part_deri_q_loss =  hession_loss_per_q.evalf(data_precision, subs={c:coeff, x_i:xi, x_j:xj, q_hat:noisy_output})\n",
    "            \n",
    "            # if the coordinate is not one of the chosen ones, then the partial derivative = 0\n",
    "            else: \n",
    "                \n",
    "                deg_2_part_deri_q_loss = 0\n",
    "            \n",
    "            # add the partial derivative of loss per every query to the total part deri\n",
    "            deg_2_part_deri_total_loss += deg_2_part_deri_q_loss\n",
    "      \n",
    "    # Now we are ready to update fake_X at x_coord_descent\n",
    "    fake_X[x_coord_descent] = fake_X[x_coord_descent] - (part_derivative[x_coord_descent]/deg_2_part_deri_total_loss)\n",
    "    \n",
    "    \n",
    "    # recalculate error for queries that were impacted\n",
    "    # save a list of queries whose error were updated bc of change in x_coord_descent\n",
    "    which_q_impacted = [] \n",
    "    for idx_q in range(k):\n",
    "            \n",
    "        # check if x_coord_descent is one of the x_i x_j used in the query\n",
    "        coords_chosen = [df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[0], df_sampled_queries[['pairs indices 1', 'pairs indices 2']].iloc[idx_q].iloc[1]]\n",
    "        bool_coord_chosen = x_coord_descent in coords_chosen    \n",
    "        # if the coordinate is in the chosen pair of x_i, x_j, we update the error for that query\n",
    "        if bool_coord_chosen == True:\n",
    "        \n",
    "            # compute query output on the current synopsis \n",
    "            fake_xi = fake_X[df_sampled_queries['pairs indices 1'].iloc[idx_q]]\n",
    "            fake_xj = fake_X[df_sampled_queries['pairs indices 2'].iloc[idx_q]]\n",
    "    \n",
    "            # compute query output on fake data \n",
    "            fake_data_output[idx_q] = deg_2_poly.evalf(data_precision, subs={c:df_sampled_queries['coefficient'].iloc[idx_q], x_i:fake_xi, x_j:fake_xj})\n",
    " \n",
    "            # update error for that query\n",
    "            # notice that this is |q(X) - real_data_noisy_output|\n",
    "            error[idx_q] = abs(fake_data_output[idx_q]-real_data_noisy_output[idx_q])\n",
    "            which_q_impacted.append(idx_q)\n",
    "    \n",
    "    # add 1 to num_iter_descent \n",
    "    num_iter_descent += 1\n",
    "    \n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "528f2206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3361024 , 0.44820516, 0.63875952, ..., 0.18567725, 0.16615223,\n",
       "       0.00613493])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6dad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part_derivative = np.zeros(num_points)\n",
    "    \n",
    "#     # compute the partial derivative of the loss function with respect to each coordinate     \n",
    "# for coordinate in range(num_points):\n",
    "#     part_derivative[coordinate] = partial_derivative_quad_loss(X, coordinate, c)\n",
    "\n",
    "# part_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7442eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query does the following:\n",
    "# Given a database X, it selects two points x_i, x_j \\in X uniformly at random and computes\n",
    "# q_c(X) = c(x_i + x_j)^2.\n",
    "# The set of queries Q consists of all such q_c with coefficient \\in (0, 1] \n",
    "# Note that we are not setting the precision of coefficients to allow a large size of Q\n",
    "\n",
    "# c, x_i, x_j = sp.symbols('c x_i x_j')\n",
    "\n",
    "# # deg_2_poly = Function('deg_2_poly')\n",
    "\n",
    "# deg_2_poly = c*(x_i+x_j)**2\n",
    "\n",
    "# gradient_deg_2_poly = sp.diff(deg_2_poly, x_i)\n",
    "\n",
    "# # evaluate a function\n",
    "# gradient_deg_2_poly.evalf(4, subs={c:1, x_i:2.32, x_j:3})\n",
    "\n",
    "# gradient_deg_2_poly(1,2,3)\n",
    "\n",
    "# def query_quadratic(data, coefficient):\n",
    "    \n",
    "#     # select a pair of x_i and x_j uniformly at random from data\n",
    "#     pair_indices = np.random.choice(len(data) , size = 2, replace = False, p = None)\n",
    "#     pair = data[pair_indices]\n",
    "#     output = coefficient*(pair[0]+pair[1])**2\n",
    "    \n",
    "#     return pair_indices, pair, output\n",
    "\n",
    "# partial derivative of the loss function wrt to a selected coordinate of X\n",
    "# this function is specific for the query q(x) = c(x_i+x_j)^2\n",
    "# we might be able to generalize this using built in derivative functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def partial_derivative_loss(X, coordinate, c):\n",
    "#     part_deri_total_loss = 0\n",
    "#     order_2_part_deri_total_loss = 0\n",
    "#     for index, coefficient in enumerate(c):\n",
    "#         # check if the coordinate wrt which we are taking the partial derivative is in the pair\n",
    "#         # if in the pair, take the partial derivative, otherwise partial derivative = 0\n",
    "#         if coordinate in pairs_indices[index,:]:\n",
    "#             x_i = pairs[index,:][0]\n",
    "#             x_j = pairs[index,:][0]\n",
    "#             part_deri_query_loss = (1/Llambda**2)*coefficient**2*4*(x_i+x_j)**3-4*noisy_output[index]*coefficient*(x_i+x_j)\n",
    "#             order_2_part_deri_query_loss = (1/Llambda**2)*12*coefficient**2*(x_i+x_j)**2-4*coefficient*noisy_output[index]\n",
    "#         else: \n",
    "#             part_deri_query_loss = 0\n",
    "#             order_2_part_deri_query_loss = 0\n",
    "#         part_deri_total_loss += part_deri_query_loss\n",
    "#         order_2_part_deri_total_loss += order_2_part_deri_query_loss\n",
    "#     return part_deri_total_loss, order_2_part_deri_total_loss\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
